{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook crawls all Investopedia articles and returns a matrix representation of the link relationships between articles. This matrix will be used for the PageRank in another Notebook.\n",
    "\n",
    "Input: The root domain of the Investopedia web page. The input stays the same and will not be user-defined. Other pages that Investopedia will most likely not work with this crawler as this crawler is specifically designed for Investopedia articles.\n",
    "\n",
    "Output: A matrix prepresentation of a graph network (with only 0s and 1). The graph represents the relationships between Investopedia articles. All relationships are one-way relationships. A relationship from article A to article B is defined as A having a link that leads to article B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "# For sleeper\n",
    "import time\n",
    "# For random sleeper times\n",
    "import random\n",
    "# For decimal numbers\n",
    "import decimal\n",
    "# For permanent disk storage of results as CSV\n",
    "import csv\n",
    "# For matrix manipulation\n",
    "import numpy as np\n",
    "# For finding duplicates, https://stackoverflow.com/questions/38386387/find-duplicates-in-a-array-list-of-integers\n",
    "import collections\n",
    "# For fileNotFound error\n",
    "import errno\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# https://stackoverflow.com/questions/44503576/selenium-python-how-to-stop-page-loading-when-certain-element-gets-loaded\n",
    "caps = DesiredCapabilities.CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"none\"  #  interactive, don't wait for the whole page to load\n",
    "driver = webdriver.Chrome(\"/home/janspoerer/code/janspoerer/page_rank_simple/chromedriver\", desired_capabilities=caps)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "root_investopedia_url = \"https://www.investopedia.com/dictionary/\"\n",
    "sleeper_time = 1.5\n",
    "crawl_everything_again = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _map_and_write_link_to_matrix_position(link, matrix):\n",
    "    pass \n",
    "    return matrix\n",
    "\n",
    "def _get_all_links_from_article():\n",
    "    pass\n",
    "\n",
    "def _open_article(article_link):\n",
    "    pass\n",
    "\n",
    "def _get_all_article_links_from_overview():\n",
    "    list_with_articles_from_this_page = []\n",
    "    article_elements = driver.find_elements_by_css_selector(\".item-title a\")\n",
    "    for article_element in article_elements:\n",
    "        list_with_articles_from_this_page.append(article_element.get_attribute(\"href\"))\n",
    "    return list_with_articles_from_this_page\n",
    "\n",
    "def _open_link_and_select_window(link):\n",
    "    try:\n",
    "        driver.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 't') \n",
    "    except:\n",
    "        driver.refresh()\n",
    "        driver.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 't') \n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    driver.get(link)\n",
    "    time.sleep(sleeper_time)\n",
    "    return\n",
    "    \n",
    "def _get_all_article_urls(root_investopdia_url):\n",
    "    list_all_urls = []\n",
    "    \n",
    "    # Open article overview (high-level overview with\n",
    "    # list of available categories sorted by starting\n",
    "    # letter of the article)\n",
    "    driver.get(root_investopedia_url)\n",
    "    \n",
    "    letter_elements = driver.find_elements_by_css_selector(\".alphabet a\")\n",
    "    \n",
    "    letter_links = []\n",
    "    for letter_element in letter_elements:\n",
    "        letter_links.append(letter_element.get_attribute(\"href\"))\n",
    "\n",
    "    for letter_link in letter_links:\n",
    "        _open_link_and_select_window(letter_link)\n",
    "        \n",
    "        # Some letters do not have several pages (because\n",
    "        # there are only few results). If this is the case,\n",
    "        # the article links are scraped right away without\n",
    "        # looping through (inexistent) sub-pages.\n",
    "        try:            \n",
    "            # This selects only the element that points to the last page\n",
    "            # for this letter. This allows us to set an upper boundary\n",
    "            # for our loop. We will stop our loop when this is reached.\n",
    "            last_element_number = driver.find_element_by_css_selector(\".layout-content div:nth-child(1) .ellipsis+ li .btn\").text                \n",
    "        except NoSuchElementException:\n",
    "            # Open, read links, and close\n",
    "            list_all_urls = list_all_urls + _get_all_article_links_from_overview()\n",
    "        else:\n",
    "            try:\n",
    "               last_element_number = int(last_element_number) \n",
    "            except:\n",
    "                list_all_urls = list_all_urls + _get_all_article_links_from_overview()\n",
    "            else:\n",
    "                page_links = []\n",
    "                for element_number in range(1, last_element_number):\n",
    "                    page_links.append(letter_link + \"?page=\" + str(element_number))\n",
    "                for page_link in page_links:\n",
    "                    # Open, read links, and close\n",
    "                    _open_link_and_select_window(page_link)\n",
    "                    all_article_links_from_overview = _get_all_article_links_from_overview()\n",
    "                    list_all_urls = list_all_urls + all_article_links_from_overview\n",
    "                    # Write to csv\n",
    "                    with open('links.csv','a') as fd: # 'a' option allows for appending. https://stackoverflow.com/questions/2363731/append-new-row-to-old-csv-file-python#2363742\n",
    "                        writer = csv.writer(fd, delimiter=',')\n",
    "                        for url in all_article_links_from_overview:\n",
    "                            print(url)\n",
    "                            writer.writerow([url])\n",
    "                    driver.execute_script(\"window.close();\")\n",
    "    print(\"Crawl successful, \", len(list_all_urls), \" articles were found.\")\n",
    "    return list_all_urls\n",
    "\n",
    "def _get_article_texts(list_all_urls):\n",
    "    # Open one tab; otherwise, the open_link_and_select_window() function will fail\n",
    "    driver.get(list_all_urls[-1])\n",
    "    time.sleep(sleeper_time)\n",
    "    for url in list_all_urls:\n",
    "        _open_link_and_select_window(url)\n",
    "        try:  \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#article-heading_2-0\")))\n",
    "            article_caption = driver.find_element_by_css_selector(\"#article-heading_2-0\").text\n",
    "            article_caption = article_caption.replace(\",\",\"\")\n",
    "            article_caption = article_caption.replace(\"'\",\"\")\n",
    "            article_caption = article_caption.replace('\"',\"\")\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#mntl-sc-page_1-0 p\")))\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#mntl-sc-page_1-0 a\")))\n",
    "            \n",
    "            # Gets links/hrefs only\n",
    "            # JAN STUCK HERE\n",
    "            article_link_elements = driver.find_elements_by_css_selector(\"#mntl-sc-page_1-0 a\")\n",
    "            string_article_links = \"\"\n",
    "            print(\"\\n\\n\", article_caption, \"with\", len(article_link_elements) ,\"elements:\\n\") # delete\n",
    "            for article_link in article_link_elements:\n",
    "                print(\"Element: \", article_link) # delete\n",
    "                print(\"Text: \", article_link.text) # delete\n",
    "                print(\"href: \", article_link.get_attribute(\"href\")) # delete\n",
    "                attrs = driver.execute_script('var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;', article_link) # delete\n",
    "                pprint(attrs) # delete\n",
    "                string_article_links = string_article_links + \";\" + article_link.get_attribute(\"href\")\n",
    "            # Remove leading semicolon\n",
    "            string_article_links = string_article_links[1:]\n",
    "            \n",
    "            current_article_row = [url, article_caption, string_article_links]\n",
    "        except Exception as e:\n",
    "            current_article_row = [url, \"ERROR FOR THIS URL\", str(e)] \n",
    "            \n",
    "        # Write to csv\n",
    "        with open('articles.csv','a') as fd: # 'a' option allows for appending. https://stackoverflow.com/questions/2363731/append-new-row-to-old-csv-file-python#2363742\n",
    "            writer = csv.writer(fd, delimiter=',')\n",
    "            writer.writerow(current_article_row)\n",
    "        driver.execute_script(\"window.close();\")\n",
    "        \n",
    "    return\n",
    "            \n",
    "def _load_csv_with_links(filename_of_csv_with_links):\n",
    "        try:\n",
    "            csvfile = open(filename_of_csv_with_links, \"rt\")\n",
    "        except:\n",
    "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), filename_of_csv_with_links)\n",
    "        csvReader = csv.reader(csvfile, delimiter=\",\")\n",
    "        list_all_urls = []\n",
    "        for row in csvReader:\n",
    "            list_all_urls.append((row[0]))\n",
    "        print(\"Loaded links successfully from csv.\", len(list_all_urls), \"links were found.\\n\\n\\n\")\n",
    "        return list_all_urls\n",
    "\n",
    "def run_crawler(root_investopedia_url,filename_of_csv_with_links=\"\"):\n",
    "    \"\"\"\n",
    "    This Notebook crawls all Investopedia articles and returns a matrix representation of the link relationships between articles. This matrix will be used for the PageRank in another Notebook.\n",
    "\n",
    "    Input: The root url of the Investopedia web page. The input stays the same and will not be user-defined. Other pages that Investopedia will most likely not work with this crawler as this crawler is specifically designed for Investopedia articles.\n",
    "\n",
    "    Output: A matrix prepresentation of a graph network (with only 0s and 1). The graph represents the relationships between Investopedia articles. All relationships are one-way relationships. A relationship from article A to article B is defined as A having a link that leads to article B.\n",
    "    \"\"\"\n",
    "    if filename_of_csv_with_links==\"\":\n",
    "        list_all_urls = _get_all_article_urls(root_investopedia_url)\n",
    "    else:\n",
    "        list_all_urls = _load_csv_with_links(filename_of_csv_with_links)\n",
    "    \n",
    "    # Check for duplicate links\n",
    "    print(\"Duplicates:\\n\", [item for item, count in collections.Counter(list_all_urls).items() if count > 1], \"\\n\\n\\n\")\n",
    "    \n",
    "    # Result is stored in a csv, return is empty\n",
    "    _get_article_texts(list_all_urls)\n",
    "    \n",
    "    matrix_representation_of_article_graph = []\n",
    "    \n",
    "    return matrix_representation_of_article_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded links successfully from csv. 12270 links were found.\n",
      "\n",
      "\n",
      "\n",
      "Duplicates:\n",
      " [] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " TZS (Tanzanian Shilling) with 11 elements:\n",
      "\n",
      "Element:  <selenium.webdriver.remote.webelement.WebElement (session=\"79313bef8e7fc6961224af143d33be92\", element=\"0.8421822799879741-4\")>\n",
      "Text:  \n",
      "href:  None\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=70.0.3538.77)\n  (Driver info: chromedriver=2.45.615279 (12b89733300bd268cff3b78fc76cb8f3a7cc44e5),platform=Linux 4.15.0-43-generic x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-5356aa40e070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_investopedia_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename_of_csv_with_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"links.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#  root_investopedia_url defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-142-ff245384bd1b>\u001b[0m in \u001b[0;36mrun_crawler\u001b[0;34m(root_investopedia_url, filename_of_csv_with_links)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Result is stored in a csv, return is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0m_get_article_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_all_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mmatrix_representation_of_article_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-ff245384bd1b>\u001b[0m in \u001b[0;36m_get_article_texts\u001b[0;34m(list_all_urls)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_article_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"window.close();\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    634\u001b[0m         return self.execute(command, {\n\u001b[1;32m    635\u001b[0m             \u001b[0;34m'script'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             'args': converted_args})['value']\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=70.0.3538.77)\n  (Driver info: chromedriver=2.45.615279 (12b89733300bd268cff3b78fc76cb8f3a7cc44e5),platform=Linux 4.15.0-43-generic x86_64)\n"
     ]
    }
   ],
   "source": [
    "run_crawler(root_investopedia_url,filename_of_csv_with_links=\"links.csv\") #  root_investopedia_url defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
